diff --git a/datasets/ACDC.py b/datasets/ACDC.py
index ecf19e7..7310540 100644
--- a/datasets/ACDC.py
+++ b/datasets/ACDC.py
@@ -30,7 +30,7 @@ class ACDC_Dataset(Dataset):
         self.test_dict = infos['test']
         self.all_dict = self.preprocess(is_train)
         self.file_list = list(self.all_dict.keys())
-        self.fineSize = [crop_length, 16, 128, 128]
+        self.fineSize = [crop_length, args.image_size[2], args.image_size[0], args.image_size[1]]
 
         self.transform = transforms.Compose([
                                 transform.ToTensorVideo(),
@@ -79,19 +79,19 @@ class ACDC_Dataset(Dataset):
         return current_video.permute(0, 2, 3, 1).unsqueeze(1)
 
     def __len__(self):
-        return len(self.file_list) * 10
+        return len(self.file_list) * 5
     
     def preprocess(self, is_train):
         all_dict = dict()
         count = 0
-        if is_train:
-            for key in list(self.train_dict.keys()):
-                all_dict[count] = (self.train_dict[key])
-                count += 1
-        else:
-            for key in list(self.test_dict.keys()):
-                all_dict[count] = (self.test_dict[key])
-                count += 1
+        # if is_train: 
+        for key in list(self.train_dict.keys()):
+            all_dict[count] = (self.train_dict[key])
+            count += 1
+        # else:
+        for key in list(self.test_dict.keys()):
+            all_dict[count] = (self.test_dict[key])
+            count += 1
         return all_dict
     
     def augment(self, img_list, hflip=True, rot=True, split='val'):
diff --git a/datasets/ACDC_test.py b/datasets/ACDC_test.py
index 03bcc71..8b0d7ef 100644
--- a/datasets/ACDC_test.py
+++ b/datasets/ACDC_test.py
@@ -30,7 +30,7 @@ class ACDC_Dataset(Dataset):
         self.test_dict = infos['test']
         self.all_dict = self.preprocess(is_train)
         self.file_list = list(self.all_dict.keys())
-        self.fineSize = [crop_length, 16, 128, 128]
+        self.fineSize = [crop_length, 32, 128, 128]
 
         self.transform = transforms.Compose([
                                 transform.ToTensorVideo(),
diff --git a/datasets/__pycache__/ACDC.cpython-38.pyc b/datasets/__pycache__/ACDC.cpython-38.pyc
index 151b07c..dafa010 100644
Binary files a/datasets/__pycache__/ACDC.cpython-38.pyc and b/datasets/__pycache__/ACDC.cpython-38.pyc differ
diff --git a/datasets/__pycache__/ACDC_test.cpython-38.pyc b/datasets/__pycache__/ACDC_test.cpython-38.pyc
index 7b0ed0b..4d6bb9e 100644
Binary files a/datasets/__pycache__/ACDC_test.cpython-38.pyc and b/datasets/__pycache__/ACDC_test.cpython-38.pyc differ
diff --git a/evaluate_acdc.py b/evaluate_acdc.py
index b555d4c..5352f87 100644
--- a/evaluate_acdc.py
+++ b/evaluate_acdc.py
@@ -37,17 +37,19 @@ class Eval:
                          mlp_dim = args.latent_dim,
                          dropout = 0.1,).to(args.device)
         
-        pretrain_params = torch.load('./results/checkpoints/checkpoint.pth', map_location='cpu')
+        pretrain_params = torch.load('./results/checkpoints/checkpoint_2.pth', map_location='cpu')
         pretrain_params = {k.replace('module.', ''): v for k, v in pretrain_params.items() if k.replace('module.', '') in self.RViT.state_dict()}
         self.RViT.load_state_dict(pretrain_params)
         
-        infos = np.load('./datasets/dataset_utils/ACDC_info.npy', allow_pickle=True).item()
+        infos = np.load('/home/jyangcu/Pulmonary_Arterial_Hypertension/datasets/ACDC_info.npy', allow_pickle=True).item()
+        # infos = np.load('./datasets/dataset_utils/ACDC_info.npy', allow_pickle=True).item()
         valid_dataset = ACDC_Dataset(args, infos)
         self.valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=1)
         self.SSIM_metric = SSIM(window_size = 8)
         self.PSNR_metric = PSNR()
 
-        self.eval(args)
+        with torch.no_grad():
+            self.eval(args)
 
     def eval(self, args):
         self.RViT.eval()
@@ -56,8 +58,9 @@ class Eval:
         all_psnr, all_dice, all_ssim, lv_dice, rv_dice, myo_dice, lv_dice = [], [], [], [], [], [], []
         pbar = tqdm(self.valid_loader)
         for step, (vids, start_anno, end_anno, es_f) in enumerate(pbar):
+            input_vids = vids.to(args.device)
             hidden = torch.zeros(1, ((args.image_size[0] * args.image_size[1] * args.image_size[2]) // (args.patch_size[0] * args.patch_size[1] * args.patch_size[2])), args.latent_dim).to(args.device)
-            _, inf_flow_all, neg_inf_flow_all, lag_flow, neg_lag_flow, lag_register, forward_regsiter, backward_regsiter = self.RViT(vids.to(args.device), hidden, train=False)
+            _, inf_flow_all, neg_inf_flow_all, lag_flow, neg_lag_flow, lag_register, forward_regsiter, backward_regsiter = self.RViT(input_vids, hidden, train=False)
 
             com_input_vids = vids.squeeze().permute(0,3,1,2)[1:, ...].cpu().mul(255)
             com_lag_register = lag_register.squeeze(0).cpu().mul(255)
@@ -84,7 +87,7 @@ class Eval:
                 # inf_flow_plt = self.plot_warpgrid(vids[0, :, idx, ...], inf_flow_all[idx][0, ...], interval=8, mark='w', heatmap=False)
                 # inf_flow_plt.savefig(f'./results/flow_result_eval/inf_flow_heatmap_warp_{idx}.png', pad_inches=0.0)
                 # inf_flow_plt.clf()
-                
+
                 # For Masks Evaluation
                 if idx == 0:
                     c_mask = start_anno
@@ -128,7 +131,6 @@ class Eval:
             # combine_imgs = torch.cat([orginial_imgs, forward_imgs, backward_imgs, lag_imgs], dim=0)
             # vutils.save_image(combine_imgs.add(1.0).mul(0.5), os.path.join("results/example_result_eval", f"example_{step}.jpg"), nrow=len(inf_flow_all))
 
-
         print("SSIM: Mean:{}, Std:{}".format(np.mean(all_ssim), np.std(all_ssim)))
         print("PSNR: Mean:{}, Std:{}".format(np.mean(all_psnr), np.std(all_psnr)))
         print("DICE: Mean:{}, Std:{}".format(np.mean(all_dice), np.std(all_dice)))
@@ -518,7 +520,7 @@ def main(rank, args):
 if __name__ == '__main__':
     parser = argparse.ArgumentParser(description="EchoNet")
     parser.add_argument('--latent-dim', type=int, default=64, help='Latent dimension n_z (default: 256)')
-    parser.add_argument('--image-size', type=tuple, default=(128, 128, 16), help='Image height and width (default: (112, 112 ,16))')
+    parser.add_argument('--image-size', type=tuple, default=(128, 128, 32), help='Image height and width (default: (112, 112 ,16))')
     parser.add_argument('--image-channels', type=int, default=1, help='Number of channels of images (default: 3)')
     parser.add_argument('--patch-size', type=int, default=(16, 16, 16), help='Patch height and width (default: 8)')
     parser.add_argument('--blurring', type=bool, default=False, help='Whether blur the image')
@@ -538,7 +540,7 @@ if __name__ == '__main__':
     parser.add_argument('--beta2', type=float, default=0.99, help='Adam beta param (default: 0.999)')
     parser.add_argument('--clip-grad', type=bool, default=True, help='perform gradient clipping in training (default: False)')
 
-    parser.add_argument('--enable_GPUs_id', type=list, default=[1], help='The number and order of the enable gpus')
+    parser.add_argument('--enable_GPUs_id', type=list, default=[2], help='The number and order of the enable gpus')
     parser.add_argument('--wandb', type=bool, default=False, help='Enable Wandb')
 
 
diff --git a/models/GPTrack_3D.py b/models/GPTrack_3D.py
index 39d9aa5..b3fb2e7 100644
--- a/models/GPTrack_3D.py
+++ b/models/GPTrack_3D.py
@@ -511,14 +511,16 @@ class RViT(nn.Module):
         tgt_X = X[:,1:]
         cat_X = torch.cat((src_X, tgt_X), dim=2)
         b, t, c, h, w, d = cat_X.shape
+
         input = rearrange(cat_X, 'b t c h w d  -> (b t) c h w d')
 
         x_enc = [input]
         for enc_layer in self.enc:
             x_enc.append(enc_layer(x_enc[-1]))
-
+        
         emb = x_enc[-1]
         _, c, p_h, p_w, d = emb.shape
+
         emb = rearrange(emb, '(b t) c h w d -> b t c (h w d)', b=b, t=t).transpose(-1, -2)
 
         sample_rate = 1
@@ -623,9 +625,12 @@ class RViT(nn.Module):
 
         if train:
             if self.lag_regular:
-                lag_gradient_loss = self.velocity_loss.gradient_loss(lag_flow) + self.velocity_loss.gradient_loss(neg_lag_flow)
-                recon_loss_lag = self.recon_loss(rearrange(tgt_X, 'b t c h w d  -> (b t) c h w d'), lag_y_src)
-                recon_loss_neg_lag = self.recon_loss(rearrange(torch.flip(src_X, dims=[1]), 'b t c h w d -> (b t) c h w d'), lag_neg_y_src)
+                # lag_gradient_loss = self.velocity_loss.gradient_loss(lag_flow) + self.velocity_loss.gradient_loss(neg_lag_flow)
+                lag_gradient_loss = 0
+                # recon_loss_lag = self.recon_loss(rearrange(tgt_X, 'b t c h w d  -> (b t) c h w d'), lag_y_src)
+                recon_loss_lag = 0
+                # recon_loss_neg_lag = self.recon_loss(rearrange(torch.flip(src_X, dims=[1]), 'b t c h w d -> (b t) c h w d'), lag_neg_y_src)
+                recon_loss_neg_lag = 0
                 # recon_loss_lag_l1 = self.velocity_loss.recon_loss(rearrange(tgt_X, 'b t c h w d -> (b t) c h w d'), lag_y_src)
                 recon_loss_lag_l1 = 0
                 # recon_loss_neg_lag_l1 = self.velocity_loss.recon_loss(rearrange(torch.flip(src_X, dims=[1]), 'b t c h w d -> (b t) c h w d'), lag_neg_y_src)
@@ -635,6 +640,8 @@ class RViT(nn.Module):
 
             return (kl_param_loss, recon_loss_tgt, recon_loss_src, smooth_loss_inf, smooth_loss_neg, lag_gradient_loss, recon_loss_lag + recon_loss_neg_lag, recon_loss_lag_l1 + recon_loss_neg_lag_l1, training_loss), \
                     inf_flow_all, neg_inf_flow_all, re_lag_flow, rearrange(lag_y_src, '(b t) c h w d -> b t c h w d', b=b, t=t), inf_regsiter, neg_inf_regsiter
+            # return (kl_param_loss, recon_loss_tgt, recon_loss_src, smooth_loss_inf, smooth_loss_neg, lag_gradient_loss, recon_loss_lag + recon_loss_neg_lag, recon_loss_lag_l1 + recon_loss_neg_lag_l1, training_loss), \
+            #         inf_flow_all, neg_inf_flow_all, re_lag_flow, rearrange(lag_y_src, '(b t) c h w d -> b t c h w d', b=b, t=t), inf_regsiter, neg_inf_regsiter
         
         return 0, inf_flow_all, neg_inf_flow_all, re_lag_flow, re_neg_lag_flow, rearrange(lag_y_src, '(b t) c h w d -> b t c h w d', b=b, t=t), inf_regsiter, neg_inf_regsiter
 
diff --git a/models/__pycache__/GPTrack_3D.cpython-38.pyc b/models/__pycache__/GPTrack_3D.cpython-38.pyc
index 5b24aea..b02996d 100644
Binary files a/models/__pycache__/GPTrack_3D.cpython-38.pyc and b/models/__pycache__/GPTrack_3D.cpython-38.pyc differ
diff --git a/train_acdc.py b/train_acdc.py
index 5488e32..01a3233 100644
--- a/train_acdc.py
+++ b/train_acdc.py
@@ -12,8 +12,12 @@ from models.GPTrack_3D import RViT
 from einops import rearrange
 from utils.tools import get_world_size, get_global_rank, get_local_rank, get_master_ip
 from datasets.ACDC import ACDC_Dataset
+from datasets.ACDC_test import ACDC_Dataset as ACDC_Dataset_test
 from monai.data import DataLoader
 
+from utils.SSIM_metric import SSIM
+from utils.PSNR_metric import PSNR
+
 import cv2
 import wandb
 import matplotlib.pyplot as plt
@@ -29,9 +33,9 @@ class Train:
                          mlp_dim = args.latent_dim,
                          dropout = 0.1,).to(args.device)
         
-        pretrain_params = torch.load('./results/checkpoints/checkpoint_0.pth', map_location='cpu')
-        pretrain_params = {k.replace('module.', ''): v for k, v in pretrain_params.items() if k.replace('module.', '') in self.RViT.state_dict()}
-        self.RViT.load_state_dict(pretrain_params)
+        # pretrain_params = torch.load('./results/checkpoints/checkpoint_0.pth', map_location='cpu')
+        # pretrain_params = {k.replace('module.', ''): v for k, v in pretrain_params.items() if k.replace('module.', '') in self.RViT.state_dict()}
+        # self.RViT.load_state_dict(pretrain_params)
         self.optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad, self.RViT.parameters()), lr=args.learning_rate, betas=[args.beta1, args.beta2])
         self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=50, gamma=0.1)
 
@@ -44,14 +48,18 @@ class Train:
         elif len(args.enable_GPUs_id) > 1:
             # For normal
             self.RViT = torch.nn.DataParallel(self.RViT, device_ids=args.enable_GPUs_id, output_device=args.enable_GPUs_id[0])
-        
-        self.mseloss = torch.nn.MSELoss()
-        self.l1loss = torch.nn.L1Loss()
+
+        self.SSIM_metric = SSIM(window_size = 8)
+        self.PSNR_metric = PSNR()
+
 
         self.prepare_training()
         infos = np.load('/home/jyangcu/Pulmonary_Arterial_Hypertension/datasets/dataset_utils/ACDC_info.npy', allow_pickle=True).item()
         train_dataset = ACDC_Dataset(args, infos)
         self.train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=8, drop_last=True)
+        infos = np.load('./datasets/dataset_utils/ACDC_info.npy', allow_pickle=True).item()
+        valid_dataset = ACDC_Dataset_test(args, infos)
+        self.valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=1)
 
         print("---- Finish the Dataset Loading ----")
 
@@ -109,10 +117,99 @@ class Train:
             # combine_imgs = torch.cat([orginial_imgs, forward_imgs, backward_imgs, lag_imgs], dim=0)
             # vutils.save_image(combine_imgs.add(1.0).mul(0.5), os.path.join("results/example_result", f"example_{epoch}_{step}.jpg"), nrow=len(inf_flow_all))
             
+            self.eval(args)
             self.scheduler.step()
             torch.save(self.RViT.state_dict(), f'./results/checkpoints/checkpoint_{epoch}.pth')
+    
+    def eval(self, args):
+        self.RViT.eval()
+        record_steps = 0
+        all_info = []
+        all_psnr, all_dice, all_ssim, lv_dice, rv_dice, myo_dice, lv_dice = [], [], [], [], [], [], []
+        pbar = tqdm(self.valid_loader)
+        for step, (vids, start_anno, end_anno, es_f) in enumerate(pbar):
+            hidden = torch.zeros(1, ((args.image_size[0] * args.image_size[1] * args.image_size[2]) // (args.patch_size[0] * args.patch_size[1] * args.patch_size[2])), args.latent_dim).to(args.device)
+            _, inf_flow_all, neg_inf_flow_all, lag_flow, neg_lag_flow, lag_register, forward_regsiter, backward_regsiter = self.RViT(vids.to(args.device), hidden, train=False)
+
+            com_input_vids = vids.squeeze().permute(0,3,1,2)[1:, ...].cpu().mul(255)
+            com_lag_register = lag_register.squeeze(0).cpu().mul(255)
+            com_forward_regsiter = torch.stack(forward_regsiter, dim=1).squeeze().permute(0,3,1,2).cpu().mul(255)
+
+            ssim_score = self.SSIM_metric(com_forward_regsiter, com_input_vids)
+            psnr_score = self.PSNR_metric(com_forward_regsiter, com_input_vids)
+            all_psnr.append(psnr_score.detach().cpu().numpy())
+            all_ssim.append(ssim_score.detach().cpu().numpy())
+
+            print("lag-reg: SSIM ---> {} , PSNR ---> {}".format(ssim_score, psnr_score))
+
+            for idx in tqdm(range(len(inf_flow_all))):
+                # inf_flow_plt = self.plot_warpgrid(vids_org[0, :, idx, ...], inf_flow_all[idx][0, ...], segment_result, interval=4, mark='r')
+                # inf_flow_plt.savefig(f'./results/flow_result_eval/inf_flow_img_warp_{idx}.png')
+                # inf_flow_plt.clf()
+                
+                # vutils.save_image(input_vids[0, :, idx, ...].add(1.0).mul(0.5), f'./results/flow_result_eval/org_img_{idx}.png')
+
+                # lag_flow_plt = self.plot_warpgrid(lag_register[0][idx], lag_flow[0, :, idx, ...], interval=4, mark='c')
+                # lag_flow_plt.savefig(f'./results/flow_result_eval/lag_flow_img_warp_{idx}.png')
+                # lag_flow_plt.clf()
+
+                # inf_flow_plt = self.plot_warpgrid(vids[0, :, idx, ...], inf_flow_all[idx][0, ...], interval=8, mark='w', heatmap=False)
+                # inf_flow_plt.savefig(f'./results/flow_result_eval/inf_flow_heatmap_warp_{idx}.png', pad_inches=0.0)
+                # inf_flow_plt.clf()
+
+                # For Masks Evaluation
+                if idx == 0:
+                    c_mask = start_anno
+                inf_flow_seg_plt, c_mask = self.plot_seg_warpgrid(vids[0, idx+1, ...], c_mask, end_anno, inf_flow_all[idx][0, ...], mark='w')
+
+                if idx == int(es_f[0]) - 2:
+                    track_segments = c_mask
+
+                # inf_flow_seg_plt.savefig(f'./results/flow_result_eval/inf_flow_seg_warp_{idx}.png',pad_inches=0.0)
+                # inf_flow_seg_plt.clf()
+
+            gt_segments = end_anno
+            gt_segments = self.transfor_label(gt_segments)
+            track_segments = self.transfor_label(track_segments)
+            pixel_acc, dice, precision, specificity, recall = self._calculate_overlap_metrics(torch.where(gt_segments > 0, 1, 0), torch.where(track_segments > 0, 1, 0))
+
+            all_dice.append(dice.detach().cpu().numpy().item())
+
+            for i in range(3):
+                i_segments_track = track_segments[i, ...]
+                i_segments_gt = gt_segments[i, ...]
+
+                _, i_dice, _, _, _ = self._calculate_overlap_metrics(i_segments_gt, i_segments_track)
+                if i == 0:
+                    rv_dice.append(i_dice.detach().cpu().numpy())
+                elif i == 1:
+                    myo_dice.append(i_dice.detach().cpu().numpy())
+                elif i == 2:
+                    lv_dice.append(i_dice.detach().cpu().numpy())
+            
+            print("Pixel Acc is : ", pixel_acc)
+            print("Dice Score is : ", dice)
+            print("Precision is : ", precision)
+            print("Specificity is : ", specificity)
+            print("Recall is : ", recall)
+            print("ES Frame Number : ", int(es_f[0])-1)
+            # orginial_imgs = input_vids[0, :, 1:, ...].transpose(0, 1)
+            # forward_imgs = torch.stack(forward_regsiter, dim=0)[:, 0, ...].detach().cpu()
+            # backward_imgs = torch.stack(backward_regsiter, dim=0)[:, 0, ...].detach().cpu()
+            # lag_imgs = lag_register[0].detach().cpu()
+            # combine_imgs = torch.cat([orginial_imgs, forward_imgs, backward_imgs, lag_imgs], dim=0)
+            # vutils.save_image(combine_imgs.add(1.0).mul(0.5), os.path.join("results/example_result_eval", f"example_{step}.jpg"), nrow=len(inf_flow_all))
 
-    def plot_warpgrid(self, args, img, warp, interval=2, show_axis=False):
+        print("SSIM: Mean:{}, Std:{}".format(np.mean(all_ssim), np.std(all_ssim)))
+        print("PSNR: Mean:{}, Std:{}".format(np.mean(all_psnr), np.std(all_psnr)))
+        print("DICE: Mean:{}, Std:{}".format(np.mean(all_dice), np.std(all_dice)))
+        print("RV_DICE: Mean:{}, Std:{}".format(np.mean(rv_dice), np.std(rv_dice)))
+        print("MYO_DICE: Mean:{}, Std:{}".format(np.mean(myo_dice), np.std(myo_dice)))
+        print("LV_DICE: Mean:{}, Std:{}".format(np.mean(lv_dice), np.std(lv_dice)))
+
+
+    def plot_seg_warpgrid(self, img, mask, mask_tgt, warp, segment_result=None, 
+                          interval=2, show_axis=False, mark='k', wrap_seg=True):
         """
         plots the given warpgrid
         @param warp: array, H x W x 2, the transformation
@@ -120,7 +217,7 @@ class Train:
         @param show_axis: Bool, should axes be included?
         @return: matplotlib plot. Show with plt.show()
         """
-        vectors = [torch.arange(0, s) for s in (args.image_size[0], args.image_size[1])]
+        vectors = [torch.arange(0, s) for s in (args.image_size[0], args.image_size[1], args.image_size[2])]
         grids = torch.meshgrid(vectors)
         grid = torch.stack(grids)  # y, x, z
         grid = torch.unsqueeze(grid, 0)  # add batch
@@ -128,35 +225,83 @@ class Train:
 
         warp = warp.unsqueeze(0).detach().cpu()
         warp = grid + warp
-        
+        warp_save = grid + warp
         shape = warp.shape[2:]
+
         for i in range(len(shape)):
             warp[:, i, ...] = 2 * (warp[:, i, ...] / (shape[i] - 1) - 0.5)
+            warp_save[:, i, ...] = (2 * (warp[:, i, ...] / (shape[i] - 1) - 0.5) - 2 * (grid[:, i] / (shape[i] - 1) - 0.5)) * shape[i]
 
         if len(shape) == 2:
             warp = warp.permute(0, 2, 3, 1)
             warp = warp[..., [1, 0]]
+
+            warp_save = warp_save.permute(0, 2, 3, 1)
+            warp_save = warp_save[..., [1, 0]]
+
         elif len(shape) == 3:
             warp = warp.permute(0, 2, 3, 4, 1)
             warp = warp[..., [2, 1, 0]]
 
-        warp = warp[0, ...].numpy()
+            warp_save = warp_save.permute(0, 2, 3, 4, 1)
+            warp_save = warp_save[..., [2, 1, 0]]
+
+        warp_save = warp_save[0, ...]
+
+        if img is not None:
+            img = img
+            mask = mask.float()
+            mask_tgt = mask_tgt.float()
+
+            # Get the warpping img
+            if wrap_seg:
+                seg_warp = torch.nn.functional.grid_sample(mask, warp, align_corners=True, mode='nearest')
+                seg_warp_a = torch.where(seg_warp == 1, 10,  seg_warp)
+                seg_warp_b = torch.where(seg_warp == 1, 10,  seg_warp)
+                seg_warp_c = torch.where(seg_warp == 1, 255, seg_warp)
+
+                seg_warp_a = torch.where(seg_warp_a == 2, 255, seg_warp_a)
+                seg_warp_b = torch.where(seg_warp_b == 2, 10,  seg_warp_b)
+                seg_warp_c = torch.where(seg_warp_c == 2, 10,  seg_warp_c)
+
+                seg_warp_a = torch.where(seg_warp_a == 3, 10,  seg_warp_a)
+                seg_warp_b = torch.where(seg_warp_b == 3, 255, seg_warp_b)
+                seg_warp_c = torch.where(seg_warp_c == 3, 10,  seg_warp_c)
 
-        img = img.transpose(1,2).add(1.0).mul(127.5).permute(1, 2, 0)
-        plt.imshow(img.detach().cpu().numpy(), cmap='gray', vmin=0, vmax=255)
+                # seg_warp_rgb = torch.cat([seg_warp_a, seg_warp_b, seg_warp_c], dim=0)
 
-        if show_axis is False:
-            plt.axis('off')
-        ax = plt.gca()
-        # ax.invert_yaxis()
-        ax.set_aspect('equal')
+                seg_mask_a = torch.where(seg_warp == mask_tgt, 0, seg_warp_a)
+                seg_mask_b = torch.where(seg_warp == mask_tgt, 0, seg_warp_b)
+                seg_mask_c = torch.where(seg_warp == mask_tgt, 0, seg_warp_c)
+                seg_mask = torch.cat([seg_mask_a, seg_mask_b, seg_mask_c], dim=0)
 
-        for row in range(0, warp.shape[0], interval):
-            plt.plot(warp[row, :, 1], warp[row, :, 0], 'c')
-        for col in range(0, warp.shape[1], interval):
-            plt.plot(warp[:, col, 1], warp[:, col, 0], 'c')
+                seg_mask_gt_a = torch.where(mask_tgt == 1, 10,  mask_tgt)
+                seg_mask_gt_b = torch.where(mask_tgt == 1, 10, mask_tgt)
+                seg_mask_gt_c = torch.where(mask_tgt == 1, 255, mask_tgt)
 
-        return plt
+                seg_mask_gt_a = torch.where(seg_mask_gt_a == 2, 255,  seg_mask_gt_a)
+                seg_mask_gt_b = torch.where(seg_mask_gt_b == 2, 10, seg_mask_gt_b)
+                seg_mask_gt_c = torch.where(seg_mask_gt_c == 2, 10, seg_mask_gt_c)
+
+                seg_mask_gt_a = torch.where(seg_mask_gt_a == 3, 10,  seg_mask_gt_a)
+                seg_mask_gt_b = torch.where(seg_mask_gt_b == 3, 255, seg_mask_gt_b)
+                seg_mask_gt_c = torch.where(seg_mask_gt_c == 3, 10,  seg_mask_gt_c)
+
+                seg_mask_gt = torch.cat([seg_mask_gt_a, seg_mask_gt_b, seg_mask_gt_c], dim=0)
+
+            if show_axis is False:
+                plt.axis('off')
+
+            n, w, h, d = img.shape
+            seg_mask = seg_mask[..., d//2].squeeze()
+            seg_mask_gt = seg_mask_gt[..., d//2].squeeze()
+            # seg_warp_rgb = seg_warp_rgb[..., d//2].squeeze()
+            img = img[..., d//2].expand(3,-1,-1).add(1.0).mul(127.5)
+            img = torch.where(seg_mask > 0, seg_mask, img) * 0.7 + img * 0.3
+            # img = torch.where(seg_mask_gt > 0, seg_mask_gt, img) * 0.7 + img * 0.3
+            plt.imshow(img.permute(1, 2, 0).detach().cpu().numpy().astype(np.uint8), cmap='viridis', vmin=0, vmax=255)
+
+        return plt, seg_warp
     
     def draw_grid(self, img, grid_height = 6, grid_width = 6, line_width=5):
         height, width, _ = img.shape
@@ -172,6 +317,29 @@ class Train:
 
         return img
 
+    def _calculate_overlap_metrics(self, gt, pred, eps=1e-5):
+        output = pred.reshape(-1, )
+        target = gt.reshape(-1, ).float()
+
+        tp = torch.sum(output * target)  # TP
+        fp = torch.sum(output * (1 - target))  # FP
+        fn = torch.sum((1 - output) * target)  # FN
+        tn = torch.sum((1 - output) * (1 - target))  # TN
+
+        pixel_acc = (tp + tn + eps) / (tp + tn + fp + fn + eps)
+        dice = (2 * tp + eps) / (2 * tp + fp + fn + eps)
+        precision = (tp + eps) / (tp + fp + eps)
+        recall = (tp + eps) / (tp + fn + eps)
+        specificity = (tn + eps) / (tn + fp + eps)
+
+        return pixel_acc, dice, precision, specificity, recall
+
+    def transfor_label(self, seg):
+        RV  = torch.where(seg == 1, 1, 0)
+        MYO = torch.where(seg == 2, 1, 0)
+        LV  = torch.where(seg == 3, 1, 0)
+        return torch.stack([RV, MYO, LV], dim=0)
+
 def main(rank, args):
 
     def wandb_init():
@@ -225,7 +393,7 @@ def main(rank, args):
 if __name__ == '__main__':
     parser = argparse.ArgumentParser(description="EchoNet")
     parser.add_argument('--latent-dim', type=int, default=64, help='Latent dimension n_z (default: 256)')
-    parser.add_argument('--image-size', type=tuple, default=(128, 128, 16), help='Image height and width (default: (112, 112 ,16))')
+    parser.add_argument('--image-size', type=tuple, default=(128, 128, 32), help='Image height and width (default: (112, 112 ,16))')
     parser.add_argument('--image-channels', type=int, default=1, help='Number of channels of images (default: 3)')
     parser.add_argument('--patch-size', type=int, default=(16, 16, 16), help='Patch height and width (default: 8)')
     parser.add_argument('--blurring', type=bool, default=False, help='Whether blur the image')
@@ -235,7 +403,6 @@ if __name__ == '__main__':
 
     parser.add_argument('--mask-size', type=int, default=8, help='The size of mask patch (default: 16)')
     parser.add_argument('--mask-ratio', type=float, default=0.7, help='The ratio of masking area in an image (default: 0.75)')
-    parser.add_argument('--selected-view', type=list, default=['4'], help='The selected view from dataset')
     parser.add_argument('--dataset-path', type=str, default='/home/jyangcu/Dataset/PH_HK_image', help='Path to data (default: /data)')
     parser.add_argument('--batch-size', type=int, default=1, help='Input batch size for training (default: 6)')
     
@@ -245,7 +412,7 @@ if __name__ == '__main__':
     parser.add_argument('--beta2', type=float, default=0.99, help='Adam beta param (default: 0.999)')
     parser.add_argument('--clip-grad', type=bool, default=True, help='perform gradient clipping in training (default: False)')
 
-    parser.add_argument('--enable_GPUs_id', type=list, default=[6], help='The number and order of the enable gpus')
+    parser.add_argument('--enable_GPUs_id', type=list, default=[0], help='The number and order of the enable gpus')
     parser.add_argument('--wandb', type=bool, default=False, help='Enable Wandb')
 
     args = parser.parse_args()
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index 7547ccb..6111923 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20250612_185057-3u1dhxyi/logs/debug-internal.log
\ No newline at end of file
+run-20250621_004103-p0s2ftj7/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index b16a6b6..7b2bbfc 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20250612_185057-3u1dhxyi/logs/debug.log
\ No newline at end of file
+run-20250621_004103-p0s2ftj7/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index 1814cca..a453cbc 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250612_185057-3u1dhxyi
\ No newline at end of file
+run-20250621_004103-p0s2ftj7
\ No newline at end of file
